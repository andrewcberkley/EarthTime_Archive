{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for GPT paper\n",
    "\n",
    "This notebook analyses a dataset of Arxiv papers for our paper Deep learning, deep change? Mapping the development of the Artificial Intelligence General Purpose Technology\n",
    "\n",
    "**Activities**\n",
    "\n",
    "1. Load the data:\n",
    " * Paper metadata\n",
    " * Paper topic mix\n",
    " * CrunchBase metadata\n",
    " * Other metadata for geocoding\n",
    "2. Analysis\n",
    "  2. Descriptive analysis of DL as a GPT\n",
    "     * Rapid improvement: Has there been a rapid increase in the number of DL papers?\n",
    "     * Rapid diffusion in other areas: Has there been an adoption of DL in a wide number of areas?\n",
    "     * Innovation in other areas: Have the DL papers been influential where they have been adopted (i.e. do they receive more citations than other papers in their category?\n",
    "  3. Geographical aspects of a GPT\n",
    "    * Do we see shifts in the geography of activity followed by changes in concentration?\n",
    "    * We perform this analys at the international and regional level\n",
    "  4. Drivers of the geography of DL\n",
    "    * Model of DL research specialization based on research and industry co-location\n",
    "    * Comparison with other `arXiv` subjects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d75ae72555d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;31m#Geocoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mshapely\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometry\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Some imports\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "#Geocoding\n",
    "import geopandas as gp\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.stats import zscore\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import random\n",
    "\n",
    "#Imports\n",
    "#Key imports are loaded from my profile (see standard_imports.py in src folder).\n",
    "\n",
    "#Paths\n",
    "\n",
    "#Paths\n",
    "top = os.path.dirname(os.getcwd())\n",
    "\n",
    "\n",
    "#Get date for saving files\n",
    "today = datetime.datetime.today()\n",
    "\n",
    "today_str = \"_\".join([str(x) for x in [today.day,today.month,today.year]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Basic functions\n",
    "\n",
    "#Flatten list\n",
    "\n",
    "def flatten_list(a_list):\n",
    "    '''\n",
    "    Utility function that takes a list of nested elements and flattens it\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return([x for el in a_list for x in el])\n",
    "\n",
    "def flatten_freqs(a_list):\n",
    "    '''\n",
    "    Utility function that flattens a list and returns element frequencies\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #This flattens the list and returns value counts\n",
    "    return(pd.Series(flatten_list(a_list)).value_counts())\n",
    "\n",
    "\n",
    "#Functions\n",
    "def create_lq_df(df,year=None):\n",
    "    '''\n",
    "    Takes a df with cells = activity in col in row and returns a df with cells = lq\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    area_activity = df.sum(axis=0)\n",
    "    area_shares = area_activity/area_activity.sum()\n",
    "    \n",
    "    lqs = df.apply(lambda x: (x/x.sum())/area_shares, axis=1)\n",
    "    \n",
    "    if year!=None:\n",
    "        lqs['period'] = year\n",
    "    \n",
    "    return(lqs)\n",
    "\n",
    "# Paper classification\n",
    "\n",
    "class DlPaperClassification():\n",
    "    '''\n",
    "    The class takes a paper df, a topic mix and an index for the topics which contain DL.\n",
    "\n",
    "    It has a .label_papers method that takes the topic mix and categories papers into DL groups.\n",
    "    \n",
    "    It also generates a categorical variable indicating if a paper is 'specialist' (dl is top category) or \n",
    "    embedded (dl is simply present)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,papers,topic_mix,dl_var):\n",
    "        '''\n",
    "        Initialise the class with a papers file,\n",
    "        A topic mix file and a list of DL categories.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #NB we will have \n",
    "        self.papers = papers\n",
    "        self.topics = topic_mix\n",
    "        \n",
    "        #This can be more than one\n",
    "        self.dl_var = dl_var\n",
    "        \n",
    "        \n",
    "    def label_papers(self,thres=0.2):\n",
    "        '''\n",
    "        We label papers into different levels of DL activity based on the weight\n",
    "        in their topic mix\n",
    "        -present if it is above a certain threshold\n",
    "        -top if it is the top topic (not necessarily above 0.5)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load all the information we need for the analysis\n",
    "        papers = self.papers\n",
    "        topics = self.topics\n",
    "        dl_var = self.dl_var\n",
    "        \n",
    "        #Classify papers into categories\n",
    "        #Is the DL topic present?\n",
    "        dl_present = pd.Series(topics[dl_var].apply(lambda x: x>thres),\n",
    "                              name='dl_present')\n",
    "        \n",
    "        #Is the DL topic the biggest one?\n",
    "        dl_max = pd.Series(topics.idxmax(axis=1)==dl_var,name='dl_spec')\n",
    "                \n",
    "        #Concatenate all categories and index them (to concatenate with the papers in a moment)\n",
    "        dl_all_class = pd.concat([dl_present,dl_max],axis=1)\n",
    "        \n",
    "        #We define an 'embed' category if papers have dl presence but are not specialised\n",
    "        dl_all_class['dl_embed'] = (dl_all_class['dl_present']==True) & (dl_all_class['dl_spec']==False)\n",
    "        \n",
    "        dl_all_class.index = topics.index\n",
    "        \n",
    "        #Concatenate papers and our topic classification\n",
    "        papers_lab = pd.concat([papers,dl_all_class],axis=1)\n",
    "        \n",
    "        #And give them a categorical variable depending on whether they are specialist or embedded\n",
    "        papers_lab['dl_category'] = ['dl_spec' if x==True else 'dl_embed' if y==True else 'not_dl' for\n",
    "                                      x,y in zip(papers_lab['dl_spec'],papers_lab['dl_embed'])]\n",
    "    \n",
    "        #Save outputs\n",
    "        #Labels stores the labels we have created\n",
    "        self.labels = dl_all_class\n",
    "        \n",
    "        #Papers_lab stores the paper metadata labelled\n",
    "        self.papers_lab = papers_lab\n",
    "        \n",
    "        #topics_agg stores the aggregated topics (mostly for checking)\n",
    "        #self.topics_agg = topic_aggregated     \n",
    "        \n",
    "        return(self)\n",
    "\n",
    "def sense_checker(data,text_var,sample_size=10,text_size=300):\n",
    "    '''\n",
    "    This function takes a dataset, draws random samples from it and prints the text so we can sense-check the quality of the matches.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Draw a random sample of size sample size from the data\n",
    "    drawn = random.sample(list(data.index),sample_size)\n",
    "    \n",
    "    #return(data.loc[drawn])\n",
    "    \n",
    "    \n",
    "    #For each element we have drawn from the sample we print the text variable up to the parameter length\n",
    "    for obs in data.loc[drawn][text_var]:\n",
    "        print(obs[:text_size])\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "def get_descriptives(df):\n",
    "    '''\n",
    "    Function to get descriptive statistics\n",
    "    \n",
    "    ----------\n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "    \n",
    "    ---------\n",
    "    Returns\n",
    "    ----------\n",
    "    -Average number of authors, average year, average number of fields, average number of institutions, average number of citations,\n",
    "    average number of citations per year\n",
    "    \n",
    "    \n",
    "    '''    \n",
    "    \n",
    "    total = len(df)\n",
    "    years = df['year'].mean()\n",
    "    fields = df['arxiv_categories'].apply(lambda x: len(x)).mean()\n",
    "    institutes = df['institutes'].apply(lambda x: len(set(x))).mean()\n",
    "    #authors = df['institutes'].apply(lambda x: len(x)).mean()\n",
    "    citations = df['citations'].mean()\n",
    "    citations_p_year = df[['citations','year']].apply(lambda x: x['citations']/(2019-x['year']),axis=1).mean()\n",
    "    \n",
    "    \n",
    "    out = pd.DataFrame([[total,years,fields,institutes,citations,citations_p_year]],\n",
    "                      columns=['total','year_average','field_average','institute_average','citation_average','citation_p_year_average']\n",
    "                      ).apply(lambda x: np.round(x,3))\n",
    "    \n",
    "    return(out)\n",
    "    \n",
    "    \n",
    "#GPT analysis\n",
    "\n",
    "def get_cited_papers(data,citation_var,q=0.75):\n",
    "    '''\n",
    "    This function subsets a dataset returning the most cited papers of the period (based on the citation variable and the quantile)\n",
    "    \n",
    "    '''\n",
    "    #Get the quantile\n",
    "    citation_quantile = papers[citation_var].quantile(q)\n",
    "    \n",
    "    #Subset the data\n",
    "    subset_data = data.loc[data[citation_var]>=citation_quantile]\n",
    "\n",
    "    return(subset_data)\n",
    "\n",
    "    \n",
    "class DlPaperAnalysis_GPT():\n",
    "    '''\n",
    "    This class generates descriptive analyses informing our first research question: Is DL a GPT.\n",
    "    \n",
    "    It does so with three methods:\n",
    "    \n",
    "        .is_growing produces a timeseries comparing levels of activity in DL papers versus the total\n",
    "        .is_spreading estimates the diffusion of DL papers in different fields\n",
    "        .is_impactful estimates the citation rates for papers in different fields\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,papers,dl_ids):\n",
    "        ''''\n",
    "        This function is initialised with the full set of papers and the ids of DL papers\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #We label the data with the ids\n",
    "        papers['is_dl'] = ['dl' if x in dl_ids else 'not_dl' for x in papers.index]\n",
    "        \n",
    "        #Store the information\n",
    "        self.papers = papers\n",
    "        \n",
    "        #Also store the DL ids although I doubt we will do much with them\n",
    "        self.dl_ids = dl_ids\n",
    "        \n",
    "        #Extract categories (we are only interested in computer science or statistics / ML)\n",
    "        categories = [x for x in set(flatten_list(papers.arxiv_categories)) if (x[:2]=='cs') | (x=='stat.ML')]\n",
    "        self.categories=categories\n",
    "        \n",
    "    def is_growing(self,\n",
    "                   #ax,\n",
    "                   year_lims=(2000,2018),thres_year=2012,high_cited=False):\n",
    "        '''\n",
    "        This method charts levels of activity in DL and compares the importance of DL before / after a threshold year\n",
    "        \n",
    "        We also give it: \n",
    "            -year_lims to subset the x axis\n",
    "            -thres_year to compare the importance of DL before/after the threshold year\n",
    "            -high_cited subsets the data to focus on the most highly cited papers each year (its value represents the\n",
    "            position in the distribution)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load papers\n",
    "        papers = self.papers\n",
    "        \n",
    "        #Subset if we are focusing on highly cited papers\n",
    "        if high_cited!=False:\n",
    "            \n",
    "            #This loops over years and extracts the top cited papers\n",
    "            papers = pd.concat([get_cited_papers(papers.loc[papers.year==x,:],'citations',high_cited) for x in np.arange(year_lims[0],year_lims[1])])\n",
    "            \n",
    "            \n",
    "        \n",
    "        #######################\n",
    "        #1. Create a timeseries\n",
    "        #######################\n",
    "        \n",
    "        #Create timeseries (note we are subsetting this table with the year lims)\n",
    "        papers_year = pd.crosstab(papers['year'],papers['is_dl']).loc[year_lims[0]:year_lims[1]]\n",
    "        \n",
    "        #Plot\n",
    "        #papers_year.plot.bar(stacked=True,ax=ax)\n",
    "    \n",
    "        #Add titles etc\n",
    "        \n",
    "        #if high_cited==False:\n",
    "         #   title = 'Number of papers in ArXiv (DL / non DL), \\n {y0}-{y1}'.format(y0=str(year_lims[0]),y1=str(year_lims[1]))\n",
    "        #else:\n",
    "        #    title = 'Number of papers in ArXiv (DL / non DL), \\n {y0}-{y1} (top {q} citations in year)'.format(y0=str(year_lims[0]),y1=str(year_lims[1]),\n",
    "        #                                                                                                      q=str(100*high_cited)+'%')\n",
    "        \n",
    "        #ax.set_title(title,size=14)\n",
    "        \n",
    "        #Store information\n",
    "        self.papers_year = papers_year\n",
    "        \n",
    "        #############################\n",
    "        #2. Before / after comparison\n",
    "        ###############################\n",
    "        \n",
    "        #Crosstabs a boolean indicating if the year is before / after the threshold and normalise over the rows\n",
    "        ct = pd.crosstab(papers['year']>thres_year,papers['is_dl'],normalize=0)\n",
    "        \n",
    "        #We want to relabel the index of the crosstab to make the output more readable\n",
    "        y = str(thres_year)\n",
    "        \n",
    "        ct.index=['Before '+y, 'After '+y]\n",
    "        \n",
    "        self.dl_shares_change= ct\n",
    "        \n",
    "    def is_spreading(self,\n",
    "                     #ax,\n",
    "                     year_lims=(2000,2017),thres_year=2012,high_cited=False,pop_categories=False):\n",
    "        '''\n",
    "        \n",
    "        This method charts the diffusion of DL across domains. \n",
    "        One annoying aspect of this is that the papers have multiple categories with no weights. \n",
    "        We will expand the data and consider categories separately.\n",
    "        \n",
    "        pop_categories allows us to focus on the more popular categories of activity where we expect our share estimates to be more robust.\n",
    "        \n",
    "        \n",
    "        #What are the key outputs:\n",
    "        \n",
    "        #Line chart representing DL papers as a share of total in papers with different categories\n",
    "        #Line chart comparing DL papers as a share of total in different categories before / after threshold.\n",
    "        \n",
    "        #Note that the ax argument has two elements for the two figures we are drawing.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load papers\n",
    "        papers = self.papers\n",
    "        \n",
    "        #Subset if we have decided to focus on highly cited papers\n",
    "        if high_cited!=False:\n",
    "            #This loops over years and extracts the top cited papers (should probably turn this into a function)\n",
    "            papers = pd.concat([get_cited_papers(papers.loc[papers.year==x,:],'citations',high_cited) for x in np.arange(year_lims[0],year_lims[1])])\n",
    "            \n",
    "        \n",
    "        #If we are filtering to focus on popular categories\n",
    "        if pop_categories!=False:\n",
    "            #This extracts the top categories based on their frequency of appearance in the data\n",
    "            categories = flatten_freqs(papers.arxiv_categories)[self.categories][:pop_categories].index\n",
    "            \n",
    "            \n",
    "        #######\n",
    "        #1. Create linechart of activity by category\n",
    "        ########\n",
    "        \n",
    "        #We create a couple of containers to store the data\n",
    "        #Share container stores the share of DL in total (we will use this for plotting)\n",
    "        cat_share_container =[]\n",
    "        \n",
    "        #Cat total container stores the totals for each category. We use a dict for this\n",
    "        cat_total_container = {}\n",
    "        \n",
    "        \n",
    "        #We loop over each category of interest\n",
    "        for cat in categories:\n",
    "            #Subset the data to identify papers with the category\n",
    "            subset = papers.loc[[cat in x for x in papers['arxiv_categories']],:]\n",
    "            \n",
    "            #We crosstab year vs dl categorical\n",
    "            subset_year = pd.crosstab(subset['year'],subset['is_dl'])\n",
    "            \n",
    "            #Store the totals\n",
    "            cat_total_container[cat] = subset_year\n",
    "            \n",
    "            #If there are any DL papers at all\n",
    "            if 'dl' in subset_year.columns:\n",
    "                #Calculate the share of DL papers\n",
    "                subset_year['share'] = subset_year['dl']/subset_year.sum(axis=1)\n",
    "                \n",
    "                #We only output the share as a series named after the category (this will become the column name when we concatenate latewr)\n",
    "                out = pd.Series(subset_year['share'],name=cat)\n",
    "                \n",
    "                #Out it comes\n",
    "                cat_share_container.append(out)\n",
    "            \n",
    "        #Create the df filling nas and focusing on our threshold years\n",
    "        category_share_df = pd.concat(cat_share_container,axis=1).fillna(0).loc[year_lims[0]:year_lims[1]]\n",
    "        \n",
    "        \n",
    "        #Now we plot this.\n",
    "        #Note that we are assuming that there are too many variables for a legend. We will probably create a cleaner version with nicer labels later.\n",
    "        #category_share_df.rolling(window=3).mean().plot(legend=False,color='mediumblue',alpha=0.7,ax=ax[0])\n",
    "        \n",
    "        #ax[0].set_title('DL paper shares by ArXiv categories',size=14)\n",
    "        #ax[0].set_ylabel('Share of all papers in category /year')\n",
    "        \n",
    "        #Store results\n",
    "        self.cat_totals = cat_total_container\n",
    "        self.cat_shares = cat_share_container\n",
    "        self.cat_shares_df = category_share_df\n",
    "        \n",
    "        #########\n",
    "        #2. Create barchart comparing two intervals\n",
    "        #########\n",
    "\n",
    "        cat_period_container = []\n",
    "\n",
    "        #As before, we loop over categories.\n",
    "        for cat in categories:\n",
    "                #Subset the data to identify papers with the category\n",
    "                subset = papers.loc[[cat in x for x in papers['arxiv_categories']],:]\n",
    "\n",
    "                #We crosstab a boolean (before / after threshold) vs the dl boolean\n",
    "                subset_ct = pd.crosstab(subset['year']>thres_year,subset['is_dl'],normalize=0)\n",
    "                \n",
    "                #This is to relabel the index (useful for the chart later)\n",
    "                y = str(thres_year)\n",
    "                subset_ct.index=['Before '+y, 'After '+y]\n",
    "        \n",
    "\n",
    "                #We append to the container, turning into a series so we can rename\n",
    "                cat_period_container.append(pd.Series(subset_ct['dl'],name=cat))\n",
    "\n",
    "        #Create the df\n",
    "        cat_thres_df = pd.concat(cat_period_container,axis=1).T.sort_values('After '+y,ascending=True)\n",
    "        \n",
    "        #cat_thres_df.plot.bar(ax=ax[1])\n",
    "        \n",
    "        #ax[1].set_title('Change in DL shares before/after '+str(thres_year),size=14)\n",
    "        #ax[1].set_ylabel('Share of all papers in category/year')\n",
    "        \n",
    "        \n",
    "        #Store the df\n",
    "        \n",
    "        self.cat_thres_df = cat_thres_df\n",
    "            \n",
    "        \n",
    "    def is_impactful(self,\n",
    "                     #ax,\n",
    "                     q=0.75,year_thres=2012,pop_categories=False):\n",
    "        '''\n",
    "        Finally, we want to check if DL papers are 'impactful' - do they tend to receive more citations than other papers in each field?\n",
    "        \n",
    "        To measure this we will estimate, for each category, what is the share of DL papers in total vs share of highly cited Dl papers. \n",
    "        \n",
    "        We focus on papers published from a threshold year to avoid being skewed by changes in the historical distribution of papers.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load papers and categories\n",
    "        papers = self.papers\n",
    "        categories = self.categories\n",
    "        \n",
    "        cit_cont=[]\n",
    "        \n",
    "        #If we are filtering to focus on popular categories\n",
    "        if pop_categories!=False:\n",
    "            #This extracts the top categories based on their frequency of appearance in the data\n",
    "            categories = flatten_freqs(papers.loc[papers.year>year_thres,'arxiv_categories'])[categories][:pop_categories].index\n",
    "        \n",
    "        #For each category\n",
    "        for cat in categories:\n",
    "            try:\n",
    "                #Here we have the papers since threshold (eg 2012) in the category\n",
    "                subset = papers.loc[(papers.year>year_thres) & ([cat in x for x in papers['arxiv_categories']])]\n",
    "\n",
    "                #Share of dl in all papers\n",
    "                dl_all = subset['is_dl'].value_counts(normalize=True)['dl']\n",
    "\n",
    "                #Share of dl in highly cited papers\n",
    "                #We use a previous function to subset this\n",
    "                subset_high_cited = get_cited_papers(subset,'citations',q)\n",
    "\n",
    "                dl_high_cited = subset_high_cited['is_dl'].value_counts(normalize=True)['dl']\n",
    "\n",
    "                #out = pd.Series([dl_all,dl_high_cited],index=['dl_share_all','dl_share_high_cited'],name=cat)\n",
    "\n",
    "                #We output an index which normalises the share of high cited papers by the total. \n",
    "                #It is positive if DL papers are overrepresented amont the highly cited ones\n",
    "                out = pd.Series([dl_all,dl_high_cited],index=['all','high_cited'],name=cat)\n",
    "\n",
    "\n",
    "                cit_cont.append(out)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        #Create citation df\n",
    "        citation_df = pd.concat(cit_cont,axis=1).T\n",
    "        \n",
    "        #return(cit_cont)\n",
    "        \n",
    "        #And plot it\n",
    "        #citation_df.sort_values('high_cited_total_ratio',ascending=False).plot.bar(ax=ax,legend=False)\n",
    "        \n",
    "        #Add title\n",
    "        #ax.set_title('DL paper citation \\'competitiveness\\' \\n (papers published after {y}, top {q} citations in period))'.format(\n",
    "        #    y=str(year_thres),q=str(100*q)+'%'))\n",
    "        \n",
    "        #And x label\n",
    "        #ax.set_ylabel('(DL papers share of highly cited/ \\n DL papers share of all)-1')\n",
    "        \n",
    "        \n",
    "        #Store the df\n",
    "        self.citation_impact_df = citation_df\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "# GPT figures\n",
    "\n",
    "def make_fig1_1(df,ax):\n",
    "    '''\n",
    "    Makes figure 1 for the paper\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df.plot.bar(ax=ax[0])\n",
    "    (100*df.apply(lambda x: x/x.sum(),axis=1)).plot.bar(stacked=True,ax=ax[1])\n",
    "\n",
    "    ax[1].get_legend().set_visible(False)\n",
    "\n",
    "    ax[0].set_ylabel('Papers in arXiv',size=12)\n",
    "    ax[1].set_ylabel('Papers in arXiv \\n  (% of total)',size=12)\n",
    "    ax[1].set_xlabel('')\n",
    "    \n",
    "    \n",
    "def make_fig_2(df1,df2,ax):\n",
    "    '''\n",
    "    \n",
    "    Makes figure 2 (line chart plus scatter)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    (100*df1.rolling(3).mean()).plot(ax=ax[0],alpha=0.5,linewidth=2,color='blue')\n",
    "    \n",
    "    (100*df2['Before 2012']).plot(markeredgecolor='blue',marker='o',color='powderblue',ax=ax[1],markerfacecolor='blue')\n",
    "    (100*df2['After 2012']).plot(markeredgecolor='orange',marker='o',color='bisque',ax=ax[1],markerfacecolor='orange')\n",
    "    \n",
    "    ax[1].vlines(np.arange(len(df2)),ymin=len(df2)*[0],ymax=100*df2['After 2012'],linestyle=':',)\n",
    "    \n",
    "    ax[1].set_xticks(np.arange(len(df2)))\n",
    "    ax[1].set_xticklabels(df2.index,rotation=90)\n",
    "\n",
    "    \n",
    "    ax[0].set_ylabel('DL as % of all papers \\n in each category',size=12)\n",
    "    ax[1].set_ylabel('DL as % of all papers \\n in each category',size=12)\n",
    "\n",
    "    ax[0].legend().set_visible(False)\n",
    "    ax[1].legend()\n",
    "    \n",
    "def make_fig_3(df,ax):\n",
    "    '''\n",
    "    \n",
    "    Scatter\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = 100*df\n",
    "    \n",
    "    col = ['orange' if x['high_cited']>x['all'] else 'blue' for _,x in df.iterrows()]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ax.scatter(df['all'],df['high_cited'],\n",
    "               c=col,alpha=0.8,edgecolor=col)\n",
    "\n",
    "    for x in df.index:\n",
    "\n",
    "        \n",
    "        al = df.loc[x]['high_cited']/100+0.3\n",
    "\n",
    "        ax.annotate(s=x,xy=(df.loc[x]['all'],df.loc[x]['high_cited']),size=12,alpha=al)\n",
    "\n",
    "\n",
    "    ax.plot([0,65],[0,65],linestyle=':',color='black')\n",
    "    ax.set_ylim(0,65)\n",
    "    ax.set_xlim(0,65)\n",
    "    \n",
    "    ax.set_xlabel('Share of all papers',size=18)\n",
    "    ax.set_ylabel('Share of all \\n highly cited papers',size=18)\n",
    "\n",
    "#Geo analysis\n",
    "\n",
    "def geo_dash_data(papers,shape,geo_var,high_cited=0.5,thres=2012,top_locs=20,ranking=10):\n",
    "    '''\n",
    "    Returns data for geographical analysis:\n",
    "    \n",
    "    -geopandas dfs for a geographical comparison\n",
    "    -barchart for before/after LQ comparison\n",
    "    -Yearly LQs by location for boxplot comparison\n",
    "    -Concentration of activity for concentration comparison\n",
    "    \n",
    "    '''\n",
    "    selected_places = papers.loc[papers['dl_cat']=='dl',:].groupby(geo_var).size().sort_values(ascending=False).index[:top_locs] \n",
    "    \n",
    "    \n",
    "    #If we want to focus on high cited, apply the high cited function\n",
    "    if high_cited!=False:\n",
    "        papers = get_high_cited_year(papers,high_cited,[2007,2018])\n",
    "        \n",
    "    papers['thres'] = [f'pre_{thres}' if x<thres else f'post_{thres}' for x in papers['year']]\n",
    "    \n",
    "    ###\n",
    "    #Get specialisation\n",
    "    ###\n",
    "    \n",
    "    y_1 = get_geo_spec(shape,papers.loc[papers['year']<thres,:],geo_var)\n",
    "    y_2 = get_geo_spec(shape,papers.loc[papers['year']>thres,:],geo_var)\n",
    "    \n",
    "    #return([y_1,y_2])\n",
    "    \n",
    "    ###\n",
    "    #Get changes\n",
    "    ###\n",
    "    pre_after = papers.groupby('thres').apply(lambda x: create_lq_df(pd.pivot_table(\n",
    "        x.groupby([geo_var,'dl_cat']).size().reset_index(drop=False).fillna(0),index=geo_var,columns='dl_cat',values=0))['dl'])\n",
    "    \n",
    "    comp = pd.pivot_table(pre_after.reset_index(drop=False),index=geo_var,columns='thres',values='dl').fillna(0).loc[selected_places]\n",
    "    \n",
    " \n",
    "    #return(comp)\n",
    "    \n",
    "    ###\n",
    "    #Get volatility\n",
    "    ###\n",
    "    \n",
    "    year_lqs = pd.pivot_table(papers.loc[papers.year>2008].groupby(['year']).apply(lambda x: create_lq_df(pd.pivot_table(\n",
    "        x.groupby([geo_var,'dl_cat']).size().reset_index(drop=False).fillna(0),index=geo_var,columns='dl_cat',\n",
    "        values=0))['dl']).reset_index(drop=False).fillna(0),index=geo_var,columns='year',values='dl').fillna(0)\n",
    "    \n",
    "    #return(year_lqs)\n",
    "    \n",
    "    ###\n",
    "    #Get concentration \n",
    "    ###\n",
    "    \n",
    "    conc_dl = papers.loc[(papers.dl_cat=='dl')&(\n",
    "        papers.year>2008),:].groupby('year').apply(lambda x: get_concentration(x.groupby(geo_var).size(),ranking))\n",
    "    \n",
    "    conc_all = papers.loc[(papers.year>2008),:].groupby('year').apply(lambda x: get_concentration(x.groupby(geo_var).size(),ranking))\n",
    "\n",
    "    \n",
    "    return([\n",
    "        [y_1,y_2],\n",
    "        comp,\n",
    "        year_lqs,\n",
    "        [conc_dl,conc_all]])\n",
    "\n",
    "def get_geo_spec(shape,df,geo_var):\n",
    "    '''\n",
    "    Calculates specialisation by geography\n",
    "    \n",
    "    '''\n",
    "    #We merge the shapefile with LQS by shapefile\n",
    "    spec = pd.merge(shape,create_lq_df(pd.pivot_table(df.groupby([geo_var,'dl_cat']).size().reset_index(drop=False),\n",
    "                   index=geo_var,columns='dl_cat',values=0).fillna(0)),left_on=geo_var,right_on=geo_var,how='left').fillna(0)\n",
    "    \n",
    "    spec_tots = pd.pivot_table(df.groupby([geo_var,'dl_cat']).size().reset_index(drop=False),\n",
    "                   index=geo_var,columns='dl_cat',values=0).fillna(0)\n",
    "    spec_tots.columns = [x+'_total' for x in spec_tots.columns]\n",
    "    \n",
    "    spec = spec.set_index(geo_var).join(spec_tots).fillna(0)\n",
    "    \n",
    "    return(spec)\n",
    "\n",
    "def get_concentration(vector,top):\n",
    "    '''\n",
    "    Gets the % of activity accounted by the top x organisations\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    share_captured = vector.sort_values(ascending=False)[:top].sum()/vector.sum()\n",
    "    \n",
    "    return(share_captured)\n",
    "\n",
    "def make_map(df,variable,variable_totals,ax,cmap,shade_low=True,edge=0.5,q=0.9):\n",
    "    '''\n",
    "    This makes a map.\n",
    "    We add an alpha depending on position in the decile of activity\n",
    " \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #df['alpha'] = pd.qcut(df[variable_totals],q=np.arange(0,1.1,0.1),duplicates='drop',labels=False)/10\n",
    "    df[variable] = [lq if tot>df[variable_totals].quantile(q) else 0 for lq,tot in zip(df[variable],df[variable_totals])]\n",
    "    \n",
    "    \n",
    "    #print(df[variable_totals].quantile(0.75))\n",
    "    \n",
    "    df.plot('dl',ax=ax,\n",
    "            scheme='fisher_jenks',\n",
    "            colormap=cmap,edgecolor='darkgrey',linewidth=edge,\n",
    "            legend=True\n",
    "           )\n",
    "\n",
    "    ax.get_legend().set_bbox_to_anchor((0.19,0.5))\n",
    "    ax.get_legend().set_title('Location quotient')\n",
    "    \n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "\n",
    "def make_geo_bar(df,ax):\n",
    "    \n",
    "    df.index = [x if 'South West' not in x else 'SWCDC (Singapore)' for x in df.index ]\n",
    "    \n",
    "    df['pre_2012'].plot(markeredgecolor='blue',marker='o',color='white',ax=ax,markerfacecolor='blue')\n",
    "    df['post_2012'].plot(markeredgecolor='orange',marker='o',color='white',ax=ax,markerfacecolor='orange')\n",
    "    \n",
    "\n",
    "    col = ['orange' if x>y else 'blue' for x,y in zip(df['post_2012'],df['pre_2012'])]\n",
    "    \n",
    "    ax.vlines(np.arange(len(df)),ymin=df['pre_2012'],ymax=df['post_2012'],linestyle=':',color=col)\n",
    "    ax.hlines(y=1,xmin=0,xmax=len(df),color='darkgrey',linestyle='--')\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(df)))\n",
    "    ax.set_xticklabels(df.index,rotation=90)\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('Relative comparative advantage')\n",
    "    #ax[1].set_ylabel('DL as % of all papers in each category')\n",
    "    ax.set_xlabel('')\n",
    "    \n",
    "    \n",
    "def volatility_concentration(df_vol,df_conc,ax,selected_locations,ylim=[50,70]):\n",
    "    '''\n",
    "    Volatility and concentration plots\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #df_vol.loc[selected_locations].boxplot(ax=ax[0],grid=False)\n",
    "    \n",
    "    v = ax[0].violinplot([df_vol.loc[selected_locations,col] for col in \n",
    "                df_vol.columns],\n",
    "              showextrema=True,\n",
    "              showmeans=True)\n",
    "    \n",
    "    for pc in v['bodies']:\n",
    "        pc.set_facecolor('orange')\n",
    "        pc.set_edgecolor('lightblue')\n",
    "        pc.set_alpha(0.7)\n",
    "\n",
    "    ax[0].set_xticks(np.arange(len(df_vol.columns))+1)\n",
    "    ax[0].set_xticklabels(df_vol.columns)\n",
    "    \n",
    "    (100*pd.concat(df_conc,axis=1)).plot(ax=ax[1])\n",
    "    \n",
    "    ax[1].legend(['Deep Learning','All'])\n",
    "    \n",
    "    ax[1].set_ylim(ylim[0],ylim[1])\n",
    "\n",
    "def get_high_cited_year(data,q,year_lims):\n",
    "    '''\n",
    "    This function extracts high cited papers by year (to control for citation times).\n",
    "    TODO - put this function in all classes above\n",
    "    \n",
    "    '''\n",
    "    #This loops over the years and extracts papers in the top quantile of activity.\n",
    "    \n",
    "    out = pd.concat([get_cited_papers(data.loc[data.year==x,:],'citations',\n",
    "                                      q) for x in np.arange(year_lims[0],year_lims[1])])\n",
    "    \n",
    "    return(out)\n",
    "\n",
    "def calculate_herfindahl(series):\n",
    "    '''\n",
    "    This function takes a series and returns its herfindahl index (the sum squared of the share of each observation in the total)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    herf = np.sum([(x/np.sum(series))**2 for x in series])\n",
    "    return(herf)\n",
    "\n",
    "def sort_shares_for_concentration_plot(df,cols):\n",
    "    '''\n",
    "    This function takes a df with shares of activity by area and returns a df ready for plotting to analyse concentration\n",
    "    focusing on the columns of interest\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    totals_sorted = pd.concat([\n",
    "            df.sort_values(col,ascending=False)[col].reset_index(drop=True) for col in cols],axis=1)\n",
    "        \n",
    "    shares_sorted = totals_sorted.apply(lambda x: x/x.sum()).cumsum()\n",
    "        \n",
    "    return(shares_sorted)\n",
    "\n",
    "def concentration_analysis(papers_df,level):\n",
    "    '''\n",
    "    This function takes a papers df and a level of analysis performs a concentration analysis which returns\n",
    "    a herfindahl index which returns a concentration index for the level of activity, and a df with cumulative shares\n",
    "    of activity to visualise in a plot.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Calculate totals by category (DL and not DL)\n",
    "    totals_by_cat = pd.pivot_table(papers_df.groupby([level,'is_dl']).size().reset_index(),\n",
    "                                   index=level,columns='is_dl',values=0).fillna(0)\n",
    "        \n",
    "    #And categories for the totals\n",
    "    totals_by_cat['total'] = totals_by_cat.sum(axis=1)\n",
    "        \n",
    "    #Calculate Herfindahl with a function we defined before. We are only interested in DL and the total benchmark\n",
    "    herf = totals_by_cat.apply(lambda x: calculate_herfindahl(x))[['dl','total']]\n",
    "        \n",
    "    #Store the herfindahl indices\n",
    "    \n",
    "        \n",
    "    #To visualise these columns we creata\n",
    "        \n",
    "    shares_sorted = sort_shares_for_concentration_plot(totals_by_cat,['dl','total'])\n",
    "    \n",
    "    return([herf,shares_sorted])\n",
    "\n",
    "#Similarities and specialisations\n",
    "\n",
    "def get_category_spec(df,geo_var,cat_var,sel,norm=False):\n",
    "    \"\"\"\n",
    "    \n",
    "    Steps: activity matrix -> normalise\n",
    "    \n",
    "    Takes df, location, whether we want location quotients or totals, and the sector variables to select\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    paper_act = pd.pivot_table(df.groupby(geo_var)[cat_var].apply(lambda x: flatten_freqs(x)).reset_index(drop=False),\n",
    "                               index=geo_var,columns='level_1',values=cat_var).fillna(0)[sel]\n",
    "    \n",
    "    if norm==True:\n",
    "        return(create_lq_df(paper_act))\n",
    "    else:\n",
    "        return(paper_act)\n",
    "\n",
    "    \n",
    "def get_local_similarity(act,sim,rel_var,symmetrical=True):\n",
    "    '''\n",
    "    \n",
    "    Multiplies the activity by distance and extract\n",
    "    \n",
    "    '''\n",
    "    #Multiplies by the weights, removes itself and adds\n",
    "    \n",
    "    if symmetrical==True:\n",
    "        weighted_act = act.apply(lambda x: x*sim[rel_var],axis=1).drop(rel_var,axis=1).sum(axis=1)\n",
    "    else:\n",
    "        weighted_act = act.apply(lambda x: x*sim[rel_var],axis=1).sum(axis=1)\n",
    "    \n",
    "    return(weighted_act)\n",
    "\n",
    "# Modelling\n",
    "\n",
    "def flatten_list(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])\n",
    "\n",
    "\n",
    "def dummies_from_list(list_of_categories):\n",
    "    '''\n",
    "    This function takes a list of categories and returns a df where every column is a dummie for each unique variable\n",
    "    in the category. Admittedly, the function could be nicer.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    #We concatenate a bunch of series whose indices are the names of the variables.\n",
    "    #We could have done something similar by creating DFs with one row\n",
    "    \n",
    "    cats = [x for x in set(flatten_list(list_of_categories))]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for category in cats:\n",
    "    \n",
    "        var = [category in x for x in list_of_categories]\n",
    "\n",
    "        df[category] = var\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #dummy_df = pd.concat([pd.Series({v:1 for v in obs}) for obs in list_of_categories],axis=1).T.fillna(0)\n",
    "    return(df)\n",
    "    \n",
    "    \n",
    "#Run models\n",
    "def data_temp(var,sector_var='cats',year_thres=2012):\n",
    "    '''\n",
    "    Prepares the data for a regression with a temporal dimension\n",
    "    \n",
    "    '''\n",
    "    df = papers_modelling.copy()\n",
    "    \n",
    "    #Calculate specialisation\n",
    "    df[var] = [var in x for x in df[sector_var]]\n",
    "    \n",
    "    #Target\n",
    "    act_1 = create_lq_df(pd.pivot_table(df.loc[df.year>year_thres,:].groupby(['name_en',var]).size().reset_index(drop=False),\n",
    "               index='name_en',columns=var,values=0).fillna(0))[True]\n",
    "    \n",
    "    #Controls\n",
    "    act_0 = create_lq_df(pd.pivot_table(df.loc[df.year<=year_thres,:].groupby(['name_en',var]).size().reset_index(drop=False),\n",
    "               index='name_en',columns=var,values=0).fillna(0))[True]\n",
    "    \n",
    "    #arxiv\n",
    "    #specialised\n",
    "    arxiv_act = get_category_spec(papers_admin_map.loc[papers_admin_map.year<=year_thres,:],'name_en','expanded_cats',norm=True,sel=comp_cats+['dl'])\n",
    "    arxiv_act_sp = get_local_similarity(arxiv_act,sim,'dl',symmetrical=True)\n",
    "    \n",
    "    #Total\n",
    "    arxiv_totals = np.log(papers_admin_map.loc[papers_admin_map.year<=year_thres,:].groupby('name_en').size()+0.01)\n",
    "    \n",
    "    #crunchbase\n",
    "    #specialised\n",
    "    cb_act = get_category_spec(cb.loc[cb.year<year_thres,:],'name_en','category_split_list',norm=True,sel=sel)\n",
    "    cb_act_sp= get_local_similarity(cb_act,arx_prox_df,'dl',symmetrical=False)\n",
    "    \n",
    "    #Total\n",
    "    cb_totals = np.log(cb.loc[cb.year<year_thres,:].groupby('name_en').size()+0.01)\n",
    "    \n",
    "    #Combine\n",
    "    all_data = pd.concat([act_1,act_0,\n",
    "                          arxiv_act_sp,cb_act_sp,\n",
    "                          arxiv_totals,cb_totals],axis=1,sort=False).fillna(0)\n",
    "    \n",
    "    all_data.columns = [var+'_t1',var+'_t0','arx_sp','cb_sp','arx_tot','cb_tot']\n",
    "    \n",
    "    all_data['sp_int'] =all_data['arx_sp']*all_data['cb_sp']\n",
    "    all_data['abs_int'] =all_data['arx_tot']*all_data['cb_tot']\n",
    "\n",
    "    #all_data = all_data.loc[top_locs]\n",
    "    #all_data = all_data.apply(lambda x: zscore(x))\n",
    "\n",
    "\n",
    "    return(all_data)\n",
    "\n",
    "\n",
    "def model_temp(data,var,top_locs=False,fit_model=True,\n",
    "               exog_vars=['const',\n",
    "                          'arx_sp',\n",
    "                          'cb_sp',\n",
    "                          'sp_int',\n",
    "                          'cb_tot',\n",
    "                          'arx_tot',\n",
    "                          'abs_int',\n",
    "                          'is_china']):\n",
    "    '''\n",
    "    Cross-sectional regression\n",
    "    top_locs filters locations by their quartile. 0 means using all\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    d = data.copy()\n",
    "    \n",
    "    if fit_model==False:\n",
    "        return(d)\n",
    "    \n",
    "    else:\n",
    "        if top_locs!=False:\n",
    "        #Focus on top arxiv areas?\n",
    "            top_q_regions = papers_admin_map.groupby(['name_en']).size().quantile(top_locs)\n",
    "\n",
    "\n",
    "            top_locs = papers_modelling.groupby(['name_en']).size().index[papers_modelling.groupby(['name_en']).size()>top_q_regions]\n",
    "\n",
    "            d = d.loc[top_locs]\n",
    "            \n",
    "            d = d.apply(lambda x: zscore(x))\n",
    "            \n",
    "            d['const']=1\n",
    "\n",
    "            d['country'] = [country_reg_lookup[x] for x in d.index]\n",
    "\n",
    "            d['is_china'] = [1 if x=='CN' else 0 for x in d['country']]\n",
    "    \n",
    "            \n",
    "            \n",
    "        ex = exog_vars + [var+'_t0']\n",
    "        #print(ex)\n",
    "        \n",
    "        ols = sm.OLS(endog=d[var+'_t1'],\n",
    "                     exog=d[ex]).fit(\n",
    "            #cov_type='cluster',cov_kwds={'groups':d['country']}\n",
    "        )\n",
    "\n",
    "        return([ols,d])\n",
    "    \n",
    "#Regression table\n",
    "\n",
    "def create_reg_table(mod):\n",
    "    '''\n",
    "    Creates series with coefficients, significance and standard errors\n",
    "    '''\n",
    "    \n",
    "    coef = mod.params\n",
    "    \n",
    "    for var in coef.index:\n",
    "        \n",
    "        st=  str(np.round(mod.bse[var],3))\n",
    "        \n",
    "        #print(mod.pvalues[var])\n",
    "        if mod.pvalues[var] < 0.01:\n",
    "            #coef[var] = str(np.round(coef[var],3))+'***' + f'({st})'\n",
    "            coef[var] = str(np.round(coef[var],3))+'***'\n",
    "            coef[var+'_se'] = f'({st})'\n",
    "            \n",
    "            \n",
    "        elif mod.pvalues[var] < 0.05:\n",
    "            \n",
    "            #coef[var] = str(np.round(coef[var],3))+'***' + f'({st})'\n",
    "            coef[var] = str(np.round(coef[var],3))+'**'\n",
    "            coef[var+'_se'] = f'({st})'\n",
    "            \n",
    "        elif mod.pvalues[var] < 0.1:\n",
    "            #coef[var] = str(np.round(coef[var],3))+'*' + f'({st})'\n",
    "            #coef[var] = str(np.round(coef[var],3))+'***' + f'({st})'\n",
    "            coef[var] = str(np.round(coef[var],3))+'*'\n",
    "            coef[var+'_se'] = f'({st})'\n",
    "            \n",
    "        else:\n",
    "            coef[var] = str(np.round(coef[var],3))\n",
    "            coef[var+'_se'] = f'({st})'\n",
    "    \n",
    "    coef['y']='RCA_t1'\n",
    "    coef['R^2'] = np.round(mod.rsquared_adj,3)\n",
    "    coef['n'] = mod.nobs\n",
    "    \n",
    "    #coef.index = ['$'+x+'$' for x in coef.index]\n",
    "    \n",
    "    return(coef)\n",
    "\n",
    "#Model comparisons\n",
    "        \n",
    "def reg_comp_plot(ax,temp=True,top_cats=10,**kwargs):\n",
    "    '''\n",
    "    \n",
    "    Compares model outputs for different arXiv categories\n",
    "    '''\n",
    "    \n",
    "    #print(kwargs)\n",
    "    \n",
    "    #Top categories\n",
    "    top_arxiv_cats = ['dl']+[x for x in list(flatten_freqs(papers_meta.arxiv_categories).index) if any(x.split('.')[0]==y for y in ['cs','stat'])][:top_cats]\n",
    "    \n",
    "    #print(kwargs)\n",
    "    \n",
    "    #Model outputs\n",
    "    if temp==True:\n",
    "        data = data_temp\n",
    "        mod = model_temp\n",
    "    else:\n",
    "        data = data_cross_sect\n",
    "        mod = model_cross_sect\n",
    "    \n",
    "    model_outputs = []\n",
    "\n",
    "    for cat in top_arxiv_cats:\n",
    "        #print(cat)\n",
    "\n",
    "        d = data(cat,'cats')\n",
    "        m =  mod(d,cat,top_locs=kwargs['top'],exog_vars=kwargs['exog_vars'])\n",
    "        out = [m[0].params,m[0].conf_int()]\n",
    "        model_outputs.append(out)\n",
    "    \n",
    "    #Extract coefficients from the model outputs\n",
    "    model_pars_t = pd.concat([x[0] for x in model_outputs],axis=1,sort=False).loc[kwargs['my_vars']]\n",
    "    model_pars_t.columns = top_arxiv_cats\n",
    "    \n",
    "    my_vars = kwargs['my_vars']\n",
    "    \n",
    "    #print(model_pars_t)\n",
    "    \n",
    "    #Confidence intervals\n",
    "    yerrvars_l, yerrvars_h = [pd.concat([x[1][num] for x in model_outputs],axis=1,sort=False) for num in [0,1]]\n",
    "    yerrvars_l.columns, yerrvars_h.columns = top_arxiv_cats,top_arxiv_cats\n",
    "\n",
    "    yerrvars = [[yerrvars_l.loc[v],yerrvars_h.loc[v]] for v in [my_vars]]\n",
    "    \n",
    "    #print(yerrvars)\n",
    "    \n",
    "     \n",
    "    x_n = np.arange(len(my_vars))\n",
    "\n",
    "    cols= plt.get_cmap('Accent').colors[:5]\n",
    "\n",
    "    patches = [mpatches.Patch(facecolor=c, label=l,ec='black') for c,l in zip(cols,my_vars)]\n",
    "    \n",
    "    for num,x in enumerate(top_arxiv_cats):\n",
    "        ax.bar([num+0.23*n for n in np.arange(0,len(my_vars))],height=model_pars_t.loc[my_vars,x],width=0.18,color=cols,\n",
    "               alpha=0.7,edgecolor='black',\n",
    "               yerr = np.array(yerrvars_l.loc[my_vars,x],yerrvars_h.loc[my_vars,x]), \n",
    "               align='center', \n",
    "               capsize=3\n",
    "              )\n",
    "        ax.vlines(x=num-0.15,ymin=-2.5,ymax=2.5,edgecolor='darkgrey',linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "    ax.set_xticks([x+0.21*2 for x in np.arange(len(top_arxiv_cats))])\n",
    "    ax.set_xticklabels(top_arxiv_cats,rotation=90,size=16)\n",
    "    ax.legend(handles=patches,loc='upper right',title='Category',fontsize=16,edgecolor='grey',facecolor='white',\n",
    "              ncol=len(my_vars)\n",
    "             )\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "Download the papers from [here](https://s3.eu-west-2.amazonaws.com/nesta-open-data/arxiv_ai/corex_matched_noOAG.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the papers\n",
    "\n",
    "\n",
    "papers = pd.read_json('corex_matched_noOAG.json',orient='records')\n",
    "\n",
    "papers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 247,931 papers. This is total number of CS + Stats papers in Arxiv CS papers (see methodology in paper for a description of the process used for this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Note that there are many duplicated papers. This seems to be linked to the way they have been geocoded\n",
    "len(set(papers.arxiv_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check the data\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a timestamp so we parse it, and extract the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers['date_parsed'] = [datetime.datetime.fromtimestamp(x/1000) for x in papers['date']]\n",
    "\n",
    "papers['year'] = papers['date_parsed'].apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrunchBase analysis\n",
    "\n",
    "We load a df with total levels of dl activity by sector before 2012, and the proximity DF we will use to calculate similarites.\n",
    "\n",
    "We cannot make the raw CB data available for licensing reasons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast \n",
    "\n",
    "cb = pd.read_csv(proc_data+'/cb_processed.csv')\n",
    "\n",
    "cb['category_split_list'] = [ast.literal_eval(x) for x in cb.category_split_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the data easier to work with.\n",
    "\n",
    "We will create a df with the topics and an index with the `arxiv_id` and another with other variables we are interested in.\n",
    "\n",
    "Before doing that we need to deduplicate the papers. The analysis we are doing right now focuses on the diffusion of ML in other topics, for which we don't need individual institution information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#papers_un means papers unique\n",
    "papers_un = papers.drop_duplicates('arxiv_id')\n",
    "\n",
    "papers_un.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a topic df only including those variables referring to a topic\n",
    "topics = papers_un.set_index('arxiv_id')[[x for x in papers_un.columns if 'TOPIC' in x]]\n",
    "\n",
    "topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#These are the topics. The neural network topic are 13 and 36.\n",
    "for num,x in enumerate(topics.columns):\n",
    "    print(str(num)+' '+x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DL topics are 13 and 36\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics.columns[[13,36]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#And now we create a paper metadata df.\n",
    "# Note that we have to set the index after subsetting (otherwise the subsetting can't find the arxiv id in the columns!)\n",
    "papers_meta = papers_un[[x for x in papers_un.columns if 'TOPIC' not in x]].set_index('arxiv_id')\n",
    "\n",
    "papers_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select columns of interest\n",
    "my_columns = ['arxiv_categories','arxiv_raw_summary','arxiv_title', 'citations','year','full_title','journal','institutes']\n",
    "\n",
    "#These columns are picking up the description of the papers, the institutes involved, the journal and the year.\n",
    "#I need all these things for the analysis of 'diffusion' which is coming up.\n",
    "\n",
    "papers_meta = papers_meta[my_columns]\n",
    "\n",
    "papers_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Paper classification\n",
    "Our first stage is a descriptive analysis of DL activity: in order to do this, we need to combine the paper data and the topic mix data and then label papers based on the relative importance of the DL topics. We will then plot some descriptives.\n",
    "\n",
    "We will start with a class that classifies papers depending on the presence of DL topics. Since we have two topics and it is not straightforward to combine coefficients into a single 'DL weight', we will classify the papers twice and then combine all the information to generate a DL variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run the analysis for both classes\n",
    "dl_vars = [\n",
    "    'TOPIC_learning_neural_neural network_training_machine learning_classification_trained_machine_learn_learning algorithm',\n",
    "    'TOPIC_state art_art_state_deep_convolutional_convolutional neural_convolutional neural network_deep learning_datasets_deep neural']\n",
    "\n",
    "#Each of the elements in dl classified is the output of the classification for a topic\n",
    "dl_classified = [DlPaperClassification(papers_meta,topics,var).label_papers(thres=0.5).labels for var in dl_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We create two lists of dl papers: one that appears in either topic (expansive) and one that appears in both (restrictive)\n",
    "\n",
    "#Expansive (is in both)\n",
    "papers_expansive = dl_classified[0].loc[(dl_classified[0]['dl_present']==True) | (dl_classified[1]['dl_present']==True)].index\n",
    "\n",
    "#Restrictive (is only in one)\n",
    "papers_restrictive = dl_classified[0].loc[(dl_classified[0]['dl_present']==True) & (dl_classified[1]['dl_present']==True)].index\n",
    "\n",
    "print(len(papers_expansive))\n",
    "print(len(papers_restrictive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_0 = dl_classified[0].loc[(dl_classified[0]['dl_present']==True)].index\n",
    "top_1 = dl_classified[1].loc[(dl_classified[1]['dl_present']==True)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(papers_expansive)/len(dl_classified[0]))\n",
    "print(len(papers_restrictive)/len(dl_classified[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('Expansive')\n",
    "# print('=========')\n",
    "# sense_checker(papers_meta.loc[papers_expansive],text_var='arxiv_raw_summary')\n",
    "\n",
    "\n",
    "# print('Restrictive')\n",
    "# print('=========')\n",
    "# sense_checker(papers_meta.loc[papers_restrictive],text_var='arxiv_raw_summary')\n",
    "\n",
    "# print('diff')\n",
    "# print('=========')\n",
    "# sense_checker(papers_meta.loc[set(papers_expansive)-set(papers_restrictive)],text_var='arxiv_raw_summary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_meta_desc = papers_meta.copy()\n",
    "\n",
    "both = set(top_0) & set(top_1)\n",
    "only_t0 = set(top_0)-set(top_1)\n",
    "only_t1 = set(top_1)-set(top_0)\n",
    "\n",
    "papers_meta_desc['dl_cat'] = ['both' if x in both else 't0' if x in only_t0 else\n",
    "                             't1' if x in only_t1 else 'non_dl' for x in papers_meta_desc.index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus on the expansive version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Descriptive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quick: Number of papers, average year, average number of authors, average number of institutions,\n",
    "# average number of citations, average number of fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_meta_desc = papers_meta.copy()\n",
    "\n",
    "papers_meta_desc['dl_cat'] = ['dl' if x in papers_expansive else 'non_dl' for x in papers_meta_desc.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descr = papers_meta_desc.groupby('dl_cat').apply(lambda x: get_descriptives(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp_cats = [x for x in set(flatten_list(papers_meta_desc['arxiv_categories'])) if x.split('.')[0]=='cs']+['stat.ML']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## More descriptive analysis\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7,3))\n",
    "\n",
    "\n",
    "(100*pd.pivot_table(\n",
    "    papers_meta_desc.groupby('dl_cat')['arxiv_categories'].apply(lambda x: flatten_freqs(x)/len(x)).reset_index(drop=False),\n",
    "    index='level_1',columns='dl_cat',values='arxiv_categories').loc[comp_cats].sort_values('dl',ascending=False)[:20]).plot.bar(ax=ax)\n",
    "\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('% of papers \\n in category')\n",
    "ax.legend().set_title('Category')\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Pre-processing: Bin into countries / regions\n",
    "\n",
    "\n",
    "Get the Shapefiles here [here](https://www.naturalearthdata.com).\n",
    "\n",
    " * [Countries](https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-0-countries/)\n",
    " * [Provinces](https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-1-states-provinces/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read the shapefile\n",
    "\n",
    "admin_shape = gp.read_file(ext_data+'/admin_shapefile/ne_10m_admin_1_states_provinces.shp')\n",
    "country_shape = gp.read_file('../data/external/ne_10m_admin_0_countries.shp')\n",
    "\n",
    "admin_shape['country_reg'] = [str(x)+'_'+str(y) for x,y in zip(admin_shape.iso_a2,admin_shape.name_en)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We will use a spatial join. To do this we need to create a geopandas df with the spatial coordinates\n",
    "#for each paper. We will create an individual paper id for each paper-institution pair so it's straightforward to\n",
    "#merge things later\n",
    "\n",
    "papers['paper_id'] = ['id_'+str(num)+'_'+str(match) for num,match in enumerate(papers.match_value)]\n",
    "\n",
    "#We create a geo papers df with the lat lon\n",
    "geo_paper = papers.set_index('paper_id')[['grid_lat','grid_lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some of the papers here have multiple lat lons - they are from institutions with multiple locations.\n",
    "#We will drop them from now.\n",
    "geo_paper = geo_paper.loc[[len(x)==1 for x in geo_paper['grid_lat']]]\n",
    "\n",
    "#Also drop papers with 'none' l\n",
    "geo_paper = geo_paper.loc[[x[0]!=None for x in geo_paper['grid_lat']]]\n",
    "geo_paper = geo_paper.dropna(subset=['grid_lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we turn the lat and lon into coordinates\n",
    "paper_points = geo_paper.apply(lambda x: Point([x['grid_lon'][0],x['grid_lat'][0]]),axis=1)\n",
    "\n",
    "#And create the geodataframe\n",
    "papers_geodf = gp.GeoDataFrame(geo_paper,geometry=paper_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make sure we have the same coordinates\n",
    "papers_geodf.crs= admin_shape.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#And do the spatial join - the operation indicates that we are doing a point in polygon.\n",
    "papers_geographies = gp.sjoin(papers_geodf,admin_shape,op='within')\n",
    "\n",
    "#Also with countries\n",
    "papers_countries = gp.sjoin(papers_geodf,country_shape,op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Focus on the variables we are interested in (country and region)\n",
    "papers_geo_short = pd.DataFrame(papers_geographies[['admin','name_en','country_reg']])\n",
    "\n",
    "#Merge with the papers df\n",
    "papers_all= papers.set_index('paper_id').join(papers_geo_short,how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the papers df for spatial analysis\n",
    "#Variables of interest\n",
    "\n",
    "my_vars = ['arxiv_id','title','arxiv_raw_summary','arxiv_categories',\n",
    "           'journal','citations','institutes',\n",
    "    'grid_lat','grid_lon','admin','name_en','country_reg','year']\n",
    "\n",
    "papers_spat = papers_all[my_vars].dropna(subset=['name_en'])\n",
    "\n",
    "#Remove all observations with empty geocodes\n",
    "papers_spat = papers_spat.loc[[len(x)>0 for x in papers_spat['admin']]]\n",
    "\n",
    "papers_spat['grid_lat'],papers_spat['grid_lon'] = [[x[0] for x in papers_spat[variable]] for variable in ['grid_lat','grid_lon']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_spat.rename(columns={'name_en':'region','admin':'country'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_spat['dl_cat'] = ['dl' if x in papers_expansive else 'non_dl' for x in papers_spat.arxiv_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(100*(pd.pivot_table(papers_spat.groupby(['country','dl_cat']).size().reset_index(drop=False),\n",
    "               index='country',columns='dl_cat',values=0).fillna(\n",
    "    0).apply(lambda x:x/x.sum(),axis=0))).sort_values('dl',ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7,4))\n",
    "\n",
    "\n",
    "(100*(pd.pivot_table(papers_spat.groupby(['country','dl_cat']).size().reset_index(drop=False),\n",
    "               index='country',columns='dl_cat',values=0).fillna(\n",
    "    0).apply(lambda x:x/x.sum(),axis=0))).sort_values('dl',ascending=False)[:20].plot.bar(ax=ax)\n",
    "\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Percentage of papers in country')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "\n",
    "(100*pd.pivot_table(papers_spat.groupby(['country_reg','dl_cat']).size().reset_index(drop=False),\n",
    "               index='country_reg',columns='dl_cat',values=0).fillna(\n",
    "    0).apply(lambda x:x/x.sum(),axis=0).sort_values('dl',ascending=False)[:25]).plot.bar(ax=ax)\n",
    "\n",
    "ax.set_xticklabels([x.get_text().split('_')[1] if 'South West' not in x.get_text() else 'SWCDC Singapore' for x in ax.get_xticklabels()])\n",
    "\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Percentage of papers \\n in region')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Descriptive analysis: GPTs\n",
    "\n",
    "Having selected a set of papers to work with, we address our descriptive research questions.\n",
    "\n",
    "We will build a class that addresses these questions through its methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = DlPaperAnalysis_GPT(papers_meta,papers_expansive)\n",
    "test.is_growing(year_lims=(2005,2018))\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7,5),nrows=2,sharex=True)\n",
    "\n",
    "make_fig1_1(test.papers_year,ax)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast increase of activity in ArXiv.\n",
    "\n",
    "DL appears to be growing at a faster rate, consistent with the 'rapidity' thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "100*test.dl_shares_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test2 = DlPaperAnalysis_GPT(papers_meta,papers_expansive)\n",
    "\n",
    "test2.is_growing(high_cited=0.75)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7,5),nrows=2,sharex=True)\n",
    "\n",
    "make_fig1_1(test2.papers_year,ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.is_spreading(pop_categories=40,year_lims=(2005,2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(9,6),nrows=2)\n",
    "\n",
    "make_fig_2(test.cat_shares_df,test.cat_thres_df[['Before 2012','After 2012']],ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DL is becoming more important in multiple disciplines. This includes disciplines that specialise in the development of AI technologies (eg `cs.NE` = neural networks, or `cs.AI` = AI) but also others such as Computer Vision, Computation and Language, or Information Retrieval or graphics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "impact_df = [test.is_impactful(year_thres=y,pop_categories=40,q=0.75).citation_impact_df for y in [2009,2012,2015]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,5),ncols=3,sharey=True)\n",
    "\n",
    "make_fig_3(impact_df[0],ax[0])\n",
    "\n",
    "make_fig_3(impact_df[1],ax[1])\n",
    "\n",
    "make_fig_3(impact_df[2],ax[2])\n",
    "\n",
    "ax[1].set_ylabel('')\n",
    "ax[2].set_ylabel('')\n",
    "\n",
    "[ax[n].set_title(f'Published after {y}',size=20) for n,y in zip([0,1,2],[2009,2012,2015])]\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../reports/figures/paper_v2_material/fig_10_impact.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DL papers are overrepresented in the set of influential papers for most CS disciplines with only a few exceptions (software engineering and performance)\n",
    "\n",
    "Note that some of the most popular topics for DL (in chart above) are quite low in these rankings because DL papers represent the majority of papers in them already\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion of the descriptive GPT analysis:\n",
    "\n",
    "DL seems to fulfil all the properties of a GPT: rapid growth, diffusion in multiple areas and impact (which we proxy through citations). Now we need to analyse what this means for its geography."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Geographical analysis\n",
    "\n",
    "Having studied the development and diffusion of DL, we want to analyse their geography. What are our hypotheses here?\n",
    "\n",
    "Our **hypothesis** is that there has been a disruption in the geography of DL: a change in the relative specialisations of countries.\n",
    "\n",
    "How do we analyse this? \n",
    "\n",
    "* First we do a descriptive analysis: line charts and pre/after 2012 comparison of country 'market shares' in DL.\n",
    "* Second, we do a geographical analysis using dbscan: how does the evolution of DL clustering compare with the evolution of clustering in other domains?\n",
    "\n",
    "As before, we will write a class to do this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Pre-processing: Bin into countries / regions\n",
    "\n",
    "\n",
    "NB see [here](https://medium.com/@bobhaffner/spatial-joins-in-geopandas-c5e916a763f3) for a quick tutorial on spatial joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have lost a few more (2k) observations that had missing country information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. Geographical change analysis\n",
    "\n",
    "Now we write a class that will address our spatial questions:\n",
    "\n",
    "* What is the geographical concentration of DL research compared with other research? \n",
    "* How has the geography of DL research changed compared to other fields?\n",
    "* How have the clusters evolved over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How we we create maps\n",
    "\n",
    "#Merge paper metadata with paper geographical data\n",
    "papers_admin = pd.DataFrame(papers_geographies[['admin','name_en','country_reg','geometry']])\n",
    "\n",
    "\n",
    "papers_nat = pd.DataFrame(papers_countries[['geometry','ADMIN','NAME','SUBREGION','CONTINENT']])\n",
    "papers_nat.columns = [x.lower() for x in papers_nat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge with the papers df\n",
    "papers_admin_map= papers.set_index('paper_id').join(papers_admin,how='left')\n",
    "papers_nat_map= papers.set_index('paper_id').join(papers_nat,how='left')\n",
    "\n",
    "papers_admin_map['dl_cat'],papers_nat_map['dl_cat'] = [['dl' if x in papers_expansive else 'not_dl' for x in data] for data in [papers_admin_map.arxiv_id,\n",
    "                                                                                                  papers_nat_map.arxiv_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "country_shape.columns = [x.lower() for x in country_shape.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "country_geo_outputs = geo_dash_data(papers_nat_map,country_shape,'name',ranking=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(14,7),ncols=2,gridspec_kw={'width_ratios':[1.75,1]})\n",
    "\n",
    "make_map(country_geo_outputs[0][1],'dl',variable_totals='dl_total',ax=ax[0],cmap='viridis')\n",
    "make_geo_bar(country_geo_outputs[1].sort_values('post_2012',ascending=False),ax[1])\n",
    "\n",
    "ax[0].set_title('Deep Learning specialisation after 2012',size=14)\n",
    "ax[1].set_title('Changes in specialisation before / after 2012',size=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "admin_geo_outputs = geo_dash_data(papers_admin_map,admin_shape,'name_en',ranking=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(14,7),ncols=2,gridspec_kw={'width_ratios':[1.75,1]})\n",
    "\n",
    "\n",
    "#make_map(country_geo_outputs[0][0],'dl',variable_totals='dl_total',ax=ax[0],cmap='Blues')\n",
    "make_map(admin_geo_outputs[0][1],'dl',variable_totals='dl_total',ax=ax[0],cmap='viridis',edge=0,q=0.99)\n",
    "make_geo_bar(admin_geo_outputs[1].sort_values('post_2012',ascending=False),ax[1])\n",
    "\n",
    "ax[0].set_title('Deep Learning specialisation after 2012 (countries)',size=14)\n",
    "ax[1].set_title('Changes in specialisation before / after 2012',size=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Volatility and concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2,figsize=(7,7))\n",
    "\n",
    "selected_countries = papers_nat_map.groupby('name').size().sort_values(ascending=False)[:50].index\n",
    "\n",
    "volatility_concentration(country_geo_outputs[2],country_geo_outputs[3],ax,selected_countries)\n",
    "\n",
    "ax[0].set_ylabel('RCA',size=12)\n",
    "ax[1].set_ylabel('% of activity \\n in top 10 countries',size=12)\n",
    "\n",
    "ax[0].get_xaxis().set_visible(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2,figsize=(7,7))\n",
    "\n",
    "selected_regions = papers_admin_map.groupby('name_en').size().sort_values(ascending=False)[:150].index\n",
    "\n",
    "volatility_concentration(admin_geo_outputs[2],admin_geo_outputs[3],ax,selected_regions,ylim=[30,70])\n",
    "\n",
    "ax[0].set_ylabel('RCA',size=12)\n",
    "ax[1].set_ylabel('% of activity \\n in top 30 regions',size=12)\n",
    "\n",
    "ax[0].get_xaxis().set_visible(False)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional feature engineering\n",
    "\n",
    "Model\n",
    "\n",
    "$Dl_{t1} = \\beta_1Dl_{t1} + \\beta_2arX_{esp} + \\beta_3cb_{esp} + \\beta_4arx_{tot} + \\beta_5cb_{tot} + \\beta_6cb_{esp}arx_esp + \\beta_7arx_{tot}cb_{tot} + ...$\n",
    "\n",
    "In order to determine which are the relevant arxiv and CB categories, we need to do some additional feature engineering.\n",
    "\n",
    "In the case of arxiv, we will look for similar computer science categories based on similarities between words.\n",
    "\n",
    "In the case of cb, we will look for similar sectoral categories based on a machine learning analysis of keywords that predict labels in CB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ArXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the DL similarity vector based on topic co-occurrences in papers.\n",
    "\n",
    "#All this creates a df with the \n",
    "#comp_cats = [x for x in set(flatten_list(papers_meta.arxiv_categories)) if any(var == x.split('.')[0] for var in ['cs','stat'])]\n",
    "\n",
    "arxiv_cats_df = pd.DataFrame()\n",
    "\n",
    "for comp in comp_cats:\n",
    "    \n",
    "    var = [comp in x for x in papers_meta.arxiv_categories]\n",
    "    \n",
    "    arxiv_cats_df[comp] = var\n",
    "\n",
    "arxiv_cats_df.index= papers_meta.index\n",
    "arxiv_cats_df['dl'] = [ind in papers_expansive for ind in arxiv_cats_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim = pd.DataFrame(1-pairwise_distances(arxiv_cats_df.T,metric='cosine'),index=arxiv_cats_df.columns,columns=arxiv_cats_df.columns)\n",
    "\n",
    "sort_cats = sim.sort_values('dl',ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(sim.loc[sort_cats,sort_cats],ax=ax,cmap='viridis_r',linecolor='grey',linewidth=0.01)\n",
    "#ax.set_title('Similarities between computer science disciplines \\n (based on paper co-occcurrence)',size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration of correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_admin_map['expanded_cats'] = [x if y!='dl' else x+['dl'] for x,y in zip(\n",
    "    papers_admin_map['arxiv_categories'],\n",
    "    papers_admin_map['dl_cat'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_meta_desc['categories_expanded'] = papers_meta_desc['arxiv_categories']\n",
    "\n",
    "\n",
    "papers_meta_desc['categories_expanded'] = [cats + ['dl'] if dl_cat=='dl' else cats for cats,dl_cat in zip(papers_meta_desc['arxiv_categories'],\n",
    "                                                                                                               papers_meta_desc['dl_cat'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arx_prox_df = pd.read_csv(proc_data+'/arxiv_sector_proximities.csv',index_col=0)\n",
    "\n",
    "close_cats = arx_prox_df.sum(axis=1).sort_values(ascending=False).index\n",
    "close_arxiv_cats = arx_prox_df.mean(axis=0).sort_values(ascending=False).index\n",
    "\n",
    "#arx_prox_df.apply(lambda x: x/x.sum()).sort_values('dl',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paper_freqs = dict(flatten_freqs(papers_meta_desc.categories_expanded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "sns.heatmap(arx_prox_df.apply(lambda x: x/x.sum()).sort_values('dl',ascending=False).T.loc[sort_cats],ax=ax,cmap='viridis')\n",
    "\n",
    "ax.set_yticklabels(ax.get_yticklabels(),fontsize=11)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),fontsize=12)\n",
    "\n",
    "#plt.colorbar(im, use_gridspec=True)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initial prep:\n",
    "\n",
    "#Country lookup\n",
    "country_reg_lookup = {x:y for x,y in zip(admin_shape['name_en'],admin_shape['iso_a2'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Focus on top arxiv areas?\n",
    "\n",
    "top_q_regions = papers_admin_map.groupby(['name_en']).size().quantile(0.75)\n",
    "\n",
    "\n",
    "top_locs = papers_admin_map.groupby(['name_en']).size().index[papers_admin_map.groupby(['name_en']).size()>top_q_regions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_modelling = papers_admin_map.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_admin_map['expanded_cats'] = [x if y!='dl' else x + ['dl'] for x,y in zip(papers_admin_map['arxiv_categories'],\n",
    "                                                                                papers_admin_map['dl_cat'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_modelling['cats'] = [[cat for cat in x if cat!='dl'] for x in papers_modelling['arxiv_categories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_modelling['cats'] = [x+['dl'] if y=='dl' else x for x,y in zip(papers_modelling['cats'],papers_modelling['dl_cat'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sel = list(set(flatten_list(cb['category_split_list'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_t = data_temp('dl','cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indeps = [['arx_sp','arx_tot','is_china'],\n",
    "          ['arx_sp',\n",
    "           'cb_sp',\n",
    "           'arx_tot','is_china'],\n",
    "          ['arx_sp',\n",
    "           'cb_sp',\n",
    "           'sp_int','arx_tot','is_china'],\n",
    "          ['arx_sp',\n",
    "           'cb_sp',\n",
    "           'sp_int','abs_int','arx_tot','is_china']]\n",
    "\n",
    "\n",
    "dl_models = [model_temp(d_t,'dl',top_locs=0.5,exog_vars=x)[0] for x in indeps]\n",
    "\n",
    "reg_table = pd.concat([create_reg_table(x) for x in dl_models],axis=1).fillna('')\n",
    "\n",
    "reg_table = reg_table.loc[['y',\n",
    "    'dl_t0','dl_t0_se','arx_sp','arx_sp_se',\n",
    "                           'cb_sp','cb_sp_se',\n",
    "                           'sp_int','sp_int_se',\n",
    "                           'abs_int','abs_int_se',\n",
    "                           'arx_tot','arx_tot_se',\n",
    "                           'is_china','is_china_se',\n",
    "                           'R^2',\n",
    "                           'n']]\n",
    "\n",
    "reg_table.index = [x if '_se' not in x else '' for x in reg_table.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_names = {'y':'y','dl_t1':'RCA_t1',\n",
    "    'dl_t0':'RCA_t0',\n",
    "            'arx_sp':'arXiv_sp',\n",
    "            'cb_sp':'CrunchBase_sp',\n",
    "            'sp_int':'arXiv_sp*CrunchBase_sp',\n",
    "            'abs_int':'arXiv_sp*CrunchBase_tot',\n",
    "            'arx_tot':'arXiv_tot',\n",
    "            'is_china':'is_china',\n",
    "            'R^2':'R2',\n",
    "            'n':'n'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_table.index = [var_names[x] if x!='' else x for x in reg_table.index]\n",
    "             \n",
    "             \n",
    "reg_table.columns = ['Model '+str(x+1) for x in reg_table.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "\n",
    "sns.heatmap(model_temp(d_t,'dl',top_locs=0.5,exog_vars=indeps[-1])[1][['dl_t1','dl_t0']+indeps[-1]].corr(),cmap='viridis',ax=ax,annot=True)\n",
    "\n",
    "ax.set_yticklabels([var_names[x.get_text()] for x in ax.get_yticklabels()])\n",
    "ax.set_xticklabels([var_names[x.get_text()] for x in ax.get_xticklabels()],rotation=45,ha='right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kwargs = {'top':0.5,\n",
    "          'exog_vars':['const',\n",
    "                       'arx_sp',\n",
    "                       'cb_sp',\n",
    "                       'sp_int',\n",
    "                       'arx_tot',\n",
    "                       'abs_int',\n",
    "                       'is_china'],\n",
    "          'my_vars':[\n",
    "                     'arx_sp',\n",
    "                     'sp_int','abs_int','is_china']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "reg_comp_plot(ax,top_cats=10,**kwargs)\n",
    "\n",
    "ax.set_ylabel(r'$\\beta$',size=16)\n",
    "\n",
    "ax.set_ylim((-1.2,3))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../reports/figures/paper_v2_material/fig_16_comparison.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
